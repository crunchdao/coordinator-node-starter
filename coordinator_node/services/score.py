"""Score service: resolve actuals on inputs → score predictions → leaderboard."""
from __future__ import annotations

import asyncio
import logging
from datetime import datetime, timedelta, timezone
from typing import Any, Callable

from coordinator_node.entities.prediction import (
    CheckpointStatus, InputStatus, PredictionStatus, ScoreRecord, SnapshotRecord,
)

from coordinator_node.db.repositories import (
    DBInputRepository, DBLeaderboardRepository, DBMerkleCycleRepository,
    DBMerkleNodeRepository, DBModelRepository,
    DBPredictionRepository, DBScoreRepository, DBSnapshotRepository,
)
from coordinator_node.merkle.service import MerkleService
from coordinator_node.crunch_config import CrunchConfig
from coordinator_node.services.feed_reader import FeedReader


class ScoreService:
    def __init__(
        self,
        checkpoint_interval_seconds: int,
        scoring_function: Callable[[dict[str, Any], dict[str, Any]], dict[str, Any]],
        feed_reader: FeedReader | None = None,
        input_repository: DBInputRepository | None = None,
        prediction_repository: DBPredictionRepository | None = None,
        score_repository: DBScoreRepository | None = None,
        snapshot_repository: DBSnapshotRepository | None = None,
        model_repository: DBModelRepository | None = None,
        leaderboard_repository: DBLeaderboardRepository | None = None,
        merkle_cycle_repository: DBMerkleCycleRepository | None = None,
        merkle_node_repository: DBMerkleNodeRepository | None = None,
        contract: CrunchConfig | None = None,
        score_interval_seconds: int | None = None,
        **kwargs: Any,
    ):
        self.checkpoint_interval_seconds = checkpoint_interval_seconds
        self.score_interval_seconds = score_interval_seconds or min(60, checkpoint_interval_seconds)
        self.scoring_function = scoring_function
        self.feed_reader = feed_reader
        self.input_repository = input_repository
        self.prediction_repository = prediction_repository
        self.score_repository = score_repository
        self.snapshot_repository = snapshot_repository
        self.model_repository = model_repository
        self.leaderboard_repository = leaderboard_repository
        self.contract = contract or CrunchConfig()

        # Merkle tamper evidence
        if merkle_cycle_repository and merkle_node_repository:
            self.merkle_service: MerkleService | None = MerkleService(
                merkle_cycle_repository=merkle_cycle_repository,
                merkle_node_repository=merkle_node_repository,
            )
        else:
            self.merkle_service = None

        self.logger = logging.getLogger(__name__)
        self.stop_event = asyncio.Event()

    async def run(self) -> None:
        self.logger.info(
            "score service started (score_interval=%ds, checkpoint_interval=%ds)",
            self.score_interval_seconds, self.checkpoint_interval_seconds,
        )
        while not self.stop_event.is_set():
            try:
                self.run_once()
            except asyncio.CancelledError:
                raise
            except Exception as exc:
                self.logger.exception("score loop error: %s", exc)
                self._rollback_repositories()
            try:
                await asyncio.wait_for(self.stop_event.wait(), timeout=self.score_interval_seconds)
            except asyncio.TimeoutError:
                pass

    def run_once(self) -> bool:
        now = datetime.now(timezone.utc)

        # 1. resolve actuals on inputs past their horizon
        self._resolve_inputs(now)

        # 2. score predictions whose input has actuals
        scored = self._score_predictions(now)
        if not scored:
            self.logger.info("No predictions scored this cycle")
            return False

        # 3. write snapshots (per-model period summary + multi-metric enrichment)
        cycle_snapshots = self._write_snapshots(scored, now)

        # 4. compute ensembles (if configured)
        self._compute_ensembles(scored, now)

        # 5. rebuild leaderboard from snapshots
        self._rebuild_leaderboard()
        return True

    async def shutdown(self) -> None:
        self.stop_event.set()

    # ── 1. resolve actuals on inputs ──

    def _resolve_inputs(self, now: datetime) -> int:
        if self.input_repository is None or self.feed_reader is None:
            return 0

        unresolved = self.input_repository.find(
            status=InputStatus.RECEIVED, resolvable_before=now,
        )
        if not unresolved:
            return 0

        resolved = 0
        for inp in unresolved:
            # Query feed records using the input's scope dimensions + time window
            records = self.feed_reader.fetch_window(
                start=inp.received_at,
                end=inp.resolvable_at,
                source=inp.scope.get("source"),
                subject=inp.scope.get("subject"),
                kind=inp.scope.get("kind"),
                granularity=inp.scope.get("granularity"),
            )

            actuals = self.contract.resolve_ground_truth(records)
            if actuals is None:
                continue

            inp.actuals = actuals
            inp.status = InputStatus.RESOLVED
            self.input_repository.save(inp)
            resolved += 1

        if resolved:
            self.logger.info("Resolved actuals for %d inputs", resolved)
        return resolved

    # ── 2. score predictions ──

    def _score_predictions(self, now: datetime) -> list[ScoreRecord]:
        predictions = self.prediction_repository.find(status=PredictionStatus.PENDING)
        if not predictions:
            return []

        # Build input lookup for actuals
        input_ids = {p.input_id for p in predictions}
        inputs_by_id: dict[str, Any] = {}
        if self.input_repository is not None:
            for inp in self.input_repository.find(status=InputStatus.RESOLVED):
                if inp.id in input_ids:
                    inputs_by_id[inp.id] = inp

        scored: list[ScoreRecord] = []
        for prediction in predictions:
            inp = inputs_by_id.get(prediction.input_id)
            if inp is None or inp.actuals is None:
                continue  # actuals not yet available

            result = self.scoring_function(prediction.inference_output, inp.actuals)
            validated = self.contract.score_type(**result)

            score = ScoreRecord(
                id=f"SCR_{prediction.id}",
                prediction_id=prediction.id,
                result=validated.model_dump(),
                success=True,
                scored_at=now,
            )

            if self.score_repository is not None:
                self.score_repository.save(score)

            prediction.status = PredictionStatus.SCORED
            self.prediction_repository.save(prediction)
            scored.append(score)

        if scored:
            self.logger.info("Scored %d predictions", len(scored))
        return scored

    # ── 3. snapshots (with multi-metric enrichment) ──

    def _write_snapshots(self, scored: list[ScoreRecord], now: datetime) -> list[SnapshotRecord]:
        if self.snapshot_repository is None:
            return []

        # Group scores and predictions by model
        pred_map: dict[str, str] = {}  # prediction_id → model_id
        pred_by_id: dict[str, Any] = {}  # prediction_id → prediction
        predictions = self.prediction_repository.find(status=PredictionStatus.SCORED)
        for p in predictions:
            pred_map[p.id] = p.model_id
            pred_by_id[p.id] = p

        by_model_scores: dict[str, list[dict[str, Any]]] = {}
        by_model_preds: dict[str, list[dict[str, Any]]] = {}
        by_model_score_dicts: dict[str, list[dict[str, Any]]] = {}

        for score in scored:
            model_id = pred_map.get(score.prediction_id)
            if not model_id:
                continue
            by_model_scores.setdefault(model_id, []).append(score.result)

            pred = pred_by_id.get(score.prediction_id)
            if pred:
                by_model_preds.setdefault(model_id, []).append({
                    "inference_output": pred.inference_output,
                    "performed_at": pred.performed_at,
                    "scope": pred.scope,
                })
            by_model_score_dicts.setdefault(model_id, []).append({
                "result": score.result,
                "scored_at": score.scored_at,
            })

        # Build MetricsContext (shared across all model evaluations)
        from coordinator_node.metrics.context import MetricsContext
        metrics_context_base = MetricsContext(
            model_id="",  # set per-model below
            window_start=min((s.scored_at for s in scored), default=now),
            window_end=now,
            all_model_predictions=by_model_preds,
        )

        written_snapshots: list[SnapshotRecord] = []

        for model_id, results in by_model_scores.items():
            # Baseline aggregation
            summary = self.contract.aggregate_snapshot(results)

            # Multi-metric enrichment
            if self.contract.metrics:
                ctx = MetricsContext(
                    model_id=model_id,
                    window_start=metrics_context_base.window_start,
                    window_end=metrics_context_base.window_end,
                    all_model_predictions=metrics_context_base.all_model_predictions,
                    ensemble_predictions=metrics_context_base.ensemble_predictions,
                )
                metric_results = self.contract.compute_metrics(
                    self.contract.metrics,
                    by_model_preds.get(model_id, []),
                    by_model_score_dicts.get(model_id, []),
                    ctx,
                )
                summary.update(metric_results)

            snapshot = SnapshotRecord(
                id=f"SNAP_{model_id}_{now.strftime('%Y%m%d_%H%M%S')}",
                model_id=model_id,
                period_start=min(s.scored_at for s in scored if pred_map.get(s.prediction_id) == model_id),
                period_end=now,
                prediction_count=len(results),
                result_summary=summary,
                created_at=now,
            )
            self.snapshot_repository.save(snapshot)
            written_snapshots.append(snapshot)

        self.logger.info("Wrote %d snapshots", len(by_model_scores))

        # Merkle tamper evidence: commit cycle
        if self.merkle_service and written_snapshots:
            try:
                self.merkle_service.commit_cycle(written_snapshots, now)
            except Exception as exc:
                self.logger.warning("Merkle cycle commit failed: %s", exc)

        return written_snapshots

    # ── 4. ensemble computation ──

    def _compute_ensembles(self, scored: list[ScoreRecord], now: datetime) -> None:
        """Compute ensemble predictions for all enabled ensemble configs."""
        if not self.contract.ensembles:
            return

        from coordinator_node.services.ensemble import (
            apply_model_filter,
            build_ensemble_predictions,
            ensemble_model_id,
            is_ensemble_model,
        )
        from coordinator_node.metrics.context import MetricsContext

        # Gather current model predictions and metrics from latest snapshots
        predictions = self.prediction_repository.find(status=PredictionStatus.SCORED)
        pred_map: dict[str, str] = {}
        for p in predictions:
            pred_map[p.id] = p.model_id

        by_model_preds: dict[str, list[dict[str, Any]]] = {}
        for p in predictions:
            if is_ensemble_model(p.model_id):
                continue
            by_model_preds.setdefault(p.model_id, []).append({
                "inference_output": p.inference_output,
                "performed_at": p.performed_at,
                "scope": p.scope,
                "input_id": p.input_id,
                "scope_key": p.scope_key,
            })

        # Get metrics from latest snapshots
        all_snapshots = self.snapshot_repository.find() if self.snapshot_repository else []
        model_metrics: dict[str, dict[str, float]] = {}
        for snap in all_snapshots:
            if not is_ensemble_model(snap.model_id):
                model_metrics[snap.model_id] = {
                    k: float(v) for k, v in snap.result_summary.items()
                    if isinstance(v, (int, float))
                }

        ensemble_predictions_map: dict[str, list[dict[str, Any]]] = {}

        for ens_config in self.contract.ensembles:
            if not ens_config.enabled:
                continue

            # Filter models
            filtered_preds = apply_model_filter(
                ens_config.model_filter, model_metrics, by_model_preds,
            )

            if not filtered_preds:
                self.logger.info("Ensemble %r: no models after filtering", ens_config.name)
                continue

            # Compute weights
            strategy = ens_config.strategy
            if strategy is None:
                from coordinator_node.services.ensemble import inverse_variance
                strategy = inverse_variance

            weights = strategy(model_metrics, filtered_preds)

            # Build ensemble predictions
            ens_preds = build_ensemble_predictions(
                ens_config.name, weights, filtered_preds, now,
            )

            if not ens_preds:
                continue

            # Save ensemble predictions
            for ep in ens_preds:
                self.prediction_repository.save(ep)

            # Score ensemble predictions against actuals
            ens_scored: list[ScoreRecord] = []
            if self.input_repository is not None:
                for ep in ens_preds:
                    inputs = self.input_repository.find(status=InputStatus.RESOLVED)
                    inp = next((i for i in inputs if i.id == ep.input_id), None)
                    if inp and inp.actuals:
                        result = self.scoring_function(ep.inference_output, inp.actuals)
                        validated = self.contract.score_type(**result)
                        score = ScoreRecord(
                            id=f"SCR_{ep.id}",
                            prediction_id=ep.id,
                            result=validated.model_dump(),
                            success=True,
                            scored_at=now,
                        )
                        if self.score_repository is not None:
                            self.score_repository.save(score)
                        ens_scored.append(score)

            # Store ensemble prediction dicts for metrics context
            ens_pred_dicts = [
                {"inference_output": ep.inference_output, "performed_at": ep.performed_at,
                 "scope": ep.scope, "input_id": ep.input_id, "scope_key": ep.scope_key}
                for ep in ens_preds
            ]
            ensemble_predictions_map[ens_config.name] = ens_pred_dicts

            # Write ensemble snapshots
            if ens_scored and self.snapshot_repository:
                ens_model_id = ensemble_model_id(ens_config.name)
                results = [s.result for s in ens_scored]
                summary = self.contract.aggregate_snapshot(results)

                # Compute metrics for the ensemble too
                if self.contract.metrics:
                    ctx = MetricsContext(
                        model_id=ens_model_id,
                        window_start=min((s.scored_at for s in ens_scored), default=now),
                        window_end=now,
                        all_model_predictions=by_model_preds,
                        ensemble_predictions=ensemble_predictions_map,
                    )
                    ens_score_dicts = [{"result": s.result, "scored_at": s.scored_at} for s in ens_scored]
                    metric_results = self.contract.compute_metrics(
                        self.contract.metrics, ens_pred_dicts, ens_score_dicts, ctx,
                    )
                    summary.update(metric_results)

                snapshot = SnapshotRecord(
                    id=f"SNAP_{ens_model_id}_{now.strftime('%Y%m%d_%H%M%S')}",
                    model_id=ens_model_id,
                    period_start=min(s.scored_at for s in ens_scored),
                    period_end=now,
                    prediction_count=len(ens_scored),
                    result_summary=summary,
                    created_at=now,
                )
                self.snapshot_repository.save(snapshot)

            self.logger.info(
                "Ensemble %r: %d models, %d predictions, weights=%s",
                ens_config.name, len(weights), len(ens_preds),
                {m: round(w, 3) for m, w in weights.items()},
            )

    # ── 5. leaderboard ──

    def _rebuild_leaderboard(self) -> None:
        models = self.model_repository.fetch_all()
        snapshots = self.snapshot_repository.find() if self.snapshot_repository else []

        aggregated = self._aggregate_from_snapshots(snapshots, models)
        ranked = self._rank(aggregated)

        self.leaderboard_repository.save(
            ranked, meta={"generated_by": "coordinator_node.score_service"},
        )

    def _aggregate_from_snapshots(self, snapshots: list[SnapshotRecord], models: dict) -> list[dict[str, Any]]:
        now = datetime.now(timezone.utc)
        aggregation = self.contract.aggregation

        # Group snapshots by model
        by_model: dict[str, list[SnapshotRecord]] = {}
        for snap in snapshots:
            by_model.setdefault(snap.model_id, []).append(snap)

        entries: list[dict[str, Any]] = []
        for model_id, model_snapshots in by_model.items():
            metrics: dict[str, float] = {}

            # Windowed aggregation of the ranking key
            for window_name, window in aggregation.windows.items():
                cutoff = now - timedelta(hours=window.hours)
                window_snaps = [s for s in model_snapshots if self._ensure_utc(s.period_end) >= cutoff]
                if window_snaps:
                    vals = [float(s.result_summary.get(aggregation.ranking_key, 0)) for s in window_snaps]
                    metrics[window_name] = sum(vals) / len(vals)
                else:
                    metrics[window_name] = 0.0

            # Add latest multi-metric values from most recent snapshot
            latest_snap = max(model_snapshots, key=lambda s: self._ensure_utc(s.period_end))
            for key, value in latest_snap.result_summary.items():
                if key not in metrics and key in self.contract.metrics:
                    try:
                        metrics[key] = float(value)
                    except (ValueError, TypeError):
                        pass

            model = models.get(model_id)
            entry: dict[str, Any] = {
                "model_id": model_id,
                "score": {
                    "metrics": metrics,
                    "ranking": {
                        "key": aggregation.ranking_key,
                        "value": metrics.get(aggregation.ranking_key, 0.0),
                        "direction": aggregation.ranking_direction,
                    },
                },
            }
            if model:
                entry["model_name"] = model.name
                entry["cruncher_name"] = model.player_name
            entries.append(entry)

        return entries

    def _rank(self, entries: list[dict[str, Any]]) -> list[dict[str, Any]]:
        key = self.contract.aggregation.ranking_key
        reverse = self.contract.aggregation.ranking_direction == "desc"

        def sort_key(e: dict[str, Any]) -> float:
            score = e.get("score")
            if not isinstance(score, dict):
                return float("-inf")
            try:
                return float((score.get("metrics") or {}).get(key, 0.0))
            except Exception:
                return float("-inf")

        ranked = sorted(entries, key=sort_key, reverse=reverse)
        for idx, entry in enumerate(ranked, start=1):
            entry["rank"] = idx
        return ranked

    _rank_leaderboard = _rank

    @staticmethod
    def _ensure_utc(dt: datetime) -> datetime:
        """Ensure a datetime is timezone-aware (assume UTC if naive)."""
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt

    def _rollback_repositories(self) -> None:
        for name, repo in [("input", self.input_repository),
                           ("prediction", self.prediction_repository),
                           ("score", self.score_repository),
                           ("snapshot", self.snapshot_repository),
                           ("model", self.model_repository),
                           ("leaderboard", self.leaderboard_repository)]:
            rollback = getattr(repo, "rollback", None)
            if callable(rollback):
                try:
                    rollback()
                except Exception as exc:
                    self.logger.warning("Rollback failed for %s: %s", name, exc)
